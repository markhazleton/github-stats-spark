# Research Recommendations: Unified Profile Report

**Feature Branch**: `001-unified-profile-report`
**Research Date**: 2025-12-30
**Status**: Complete

## Overview

This document provides concrete, actionable research findings and recommendations for implementing a unified markdown report generator that consolidates SVG visualizations and repository analysis. Each research task includes a clear decision, rationale, alternatives considered, and implementation notes.

---

## Research Task 1: Markdown Report Templating Approaches

**Context**: We need to generate markdown reports that embed 6 SVG files and include structured repository analysis with rankings, metrics, and technology stacks.

### Decision

**Use Python f-strings with modular helper methods** for markdown generation rather than introducing Jinja2 dependency.

### Rationale

1. **Existing Pattern Alignment**: The current `report_generator.py` already uses Python string concatenation and format methods successfully (lines 25-61 demonstrate modular section-based approach). Adding Jinja2 would introduce architectural inconsistency.

2. **Complexity-to-Benefit Ratio**: For structured data output (not user-editable templates), f-strings provide equivalent functionality with zero dependencies. Jinja2 shines when non-developers need to modify templates - not applicable here.

3. **Performance**: F-strings are 30-40% faster than Jinja2 for simple variable substitution ([Engineering for Data Science](https://engineeringfordatascience.com/posts/python_string_formatting_for_data_science/)).

### Alternatives Considered

1. **Jinja2 Templates** (rejected)
   - **Pros**: Template reusability, portable files, rich control flow
   - **Cons**: New dependency, overkill for programmatic generation, breaks existing pattern
   - **When to use**: If users needed to customize report structure without code changes

2. **Template Strings (Python stdlib)** (rejected)
   - **Pros**: No dependencies, simple placeholder substitution
   - **Cons**: Less readable than f-strings, limited formatting capabilities

### Implementation Notes

**Recommended Pattern**:

```python
class UnifiedReportGenerator:
    """Generates unified markdown reports combining SVGs and analysis."""

    def generate_report(self, report_data: UnifiedReport) -> str:
        """Generate complete markdown document."""
        sections = []
        sections.append(self._generate_header(report_data))
        sections.append(self._generate_profile_overview(report_data))
        sections.append(self._generate_repository_analysis(report_data))
        sections.append(self._generate_footer(report_data))
        return "\n\n".join(sections)

    def _generate_header(self, report_data: UnifiedReport) -> str:
        """Generate header with metadata."""
        return f"""# GitHub Profile: {report_data.username}

**Generated**: {report_data.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}
**Report Version**: {report_data.version}
**Repositories Analyzed**: {report_data.total_repos}

---"""

    def _generate_profile_overview(self, report_data: UnifiedReport) -> str:
        """Generate profile overview with embedded SVGs."""
        lines = ["## Profile Overview", ""]

        # SVG embeddings with fallback text
        svg_order = ["overview", "heatmap", "streaks", "release", "languages", "fun"]
        for svg_type in svg_order:
            if svg_type in report_data.available_svgs:
                lines.append(self._embed_svg(svg_type, f"../{svg_type}.svg"))
            else:
                lines.append(f"*{svg_type.title()} visualization unavailable*")
            lines.append("")

        return "\n".join(lines)

    def _embed_svg(self, name: str, path: str) -> str:
        """Generate SVG embedding markdown."""
        return f"![{name.title()} Statistics]({path})"

    def _generate_repository_analysis(self, report_data: UnifiedReport) -> str:
        """Generate top 50 repository analysis section."""
        lines = [f"## Top {len(report_data.repositories)} Repositories", ""]

        for analysis in report_data.repositories:
            lines.append(self._format_repository(analysis))
            lines.append("")

        return "\n".join(lines)

    def _format_repository(self, analysis: RepositoryAnalysis) -> str:
        """Format single repository entry (reuse existing logic)."""
        # Leverage existing ReportGenerator._format_repository_entry
        # with enhancements for unified report context
        pass

    def _generate_footer(self, report_data: UnifiedReport) -> str:
        """Generate footer with metadata and attribution."""
        return f"""---

## Report Metadata

- **Generation Time**: {report_data.generation_time:.1f} seconds
- **SVGs Generated**: {len(report_data.available_svgs)}/6
- **Success Rate**: {report_data.success_rate:.1f}%

*Generated by [Stats Spark](https://github.com/markhazleton/github-stats-spark)*"""
```

**Key Principles**:
- **Modular sections**: Each section is a separate method returning a string
- **Reuse existing code**: Leverage `ReportGenerator._format_repository_entry()` for consistency
- **Clear separation**: Header/Profile/Analysis/Footer as distinct components
- **Fallback handling**: Graceful degradation when SVGs or data are missing

---

## Research Task 2: Workflow Orchestration Pattern

**Context**: We need to orchestrate SVG generation (existing `visualizer.py`) and repository analysis (existing `ranker.py` + `summarizer.py`) into a unified workflow that handles partial failures gracefully.

### Decision

**Sequential execution with parallel data fetching** + **continue-on-error pattern** for partial failures.

### Rationale

1. **Dependency Chain**: SVG generation requires statistics calculation which requires GitHub API data. Repository analysis requires the same API data. Therefore, data fetching must come first, but SVGs and analysis can proceed independently afterward.

2. **Partial Failure Requirement**: FR-011 and FR-012 explicitly require continuing even if some components fail. Sequential execution with try-catch blocks around each stage allows fine-grained error handling.

3. **Existing Infrastructure**: The current `cli.py` `handle_analyze()` (lines 180-426) already demonstrates this pattern successfully with progress tracking and error collection.

### Alternatives Considered

1. **Fully Parallel (asyncio)** (rejected)
   - **Pros**: Maximum speed, modern Python pattern
   - **Cons**: GitHub API is synchronous (PyGithub), adding async complexity for minimal gain, harder to debug
   - **When to use**: If we migrate to async GitHub library (e.g., `aiohttp` + direct API calls)

2. **Prefect/Temporal Orchestration** (rejected)
   - **Pros**: Built-in retry, monitoring, state management
   - **Cons**: Massive dependency overhead for simple workflow, requires external infrastructure
   - **When to use**: Multi-step ETL pipelines or distributed systems ([Python Workflow Tools](https://www.advsyscon.com/blog/workload-orchestration-tools-python/))

3. **Shared In-Memory Cache Only** (rejected)
   - **Pros**: Fastest data sharing
   - **Cons**: Doesn't survive process restarts, loses retry benefits of disk cache
   - **When to use**: Never - disk cache provides same speed with persistence

### Implementation Notes

**Recommended Pattern**:

```python
class UnifiedReportWorkflow:
    """Orchestrates unified report generation with partial failure handling."""

    def __init__(self, config: SparkConfig, cache: APICache):
        self.config = config
        self.cache = cache
        self.fetcher = GitHubFetcher(cache=cache)
        self.calculator = StatsCalculator()
        self.visualizer = StatisticsVisualizer(theme, enable_effects=True)
        self.ranker = RepositoryRanker(config=config.config.get("analyzer", {}))
        self.summarizer = RepositorySummarizer(cache=cache)
        self.report_generator = UnifiedReportGenerator()

        self.errors = []
        self.warnings = []

    def execute(self, username: str) -> UnifiedReport:
        """Execute unified report workflow with partial failure handling.

        Workflow stages:
        1. Fetch GitHub data (required - fails entire workflow if unsuccessful)
        2. Generate SVGs (optional - continues with warnings if fails)
        3. Analyze repositories (optional - continues with warnings if fails)
        4. Generate unified report (required - uses available data)

        Returns:
            UnifiedReport with generated content and error metadata
        """
        logger.info(f"Starting unified report workflow for {username}")

        # Stage 1: Fetch GitHub data (REQUIRED)
        try:
            github_data = self._fetch_github_data(username)
        except Exception as e:
            logger.error(f"Failed to fetch GitHub data: {e}")
            raise WorkflowError("Cannot proceed without GitHub data") from e

        # Stage 2: Generate SVGs (OPTIONAL - FR-011)
        available_svgs = []
        try:
            available_svgs = self._generate_svgs(username, github_data)
        except Exception as e:
            logger.warn(f"SVG generation failed: {e}")
            self.warnings.append(f"SVG generation failed: {e}")
            # Continue workflow - report will note missing visualizations

        # Stage 3: Analyze repositories (OPTIONAL - FR-012)
        repository_analyses = []
        try:
            repository_analyses = self._analyze_repositories(
                username, github_data.repositories, github_data.commit_histories
            )
        except Exception as e:
            logger.warn(f"Repository analysis failed: {e}")
            self.warnings.append(f"Repository analysis failed: {e}")
            # Continue workflow - report will show available data only

        # Stage 4: Generate unified report (REQUIRED)
        report = self._generate_unified_report(
            username=username,
            github_data=github_data,
            available_svgs=available_svgs,
            repository_analyses=repository_analyses,
        )

        report.errors = self.errors
        report.warnings = self.warnings
        report.partial_results = len(self.errors) > 0 or len(self.warnings) > 0

        return report

    def _fetch_github_data(self, username: str) -> GitHubData:
        """Fetch all required GitHub data with retry logic."""
        # Leverage existing tenacity retry decorator from summarizer.py
        @retry(
            stop=stop_after_attempt(3),
            wait=wait_exponential(multiplier=60, min=60, max=900),  # 1min, 5min, 15min
            retry=retry_if_exception_type(RateLimitException),
        )
        def fetch_with_retry():
            repos = self.fetcher.fetch_repositories(username, exclude_private=True)
            commit_histories = {}

            for repo_data in repos:
                try:
                    commits = self.fetcher.fetch_commit_counts(username, repo_data['name'])
                    commit_histories[repo_data['name']] = commits
                except Exception as e:
                    logger.warn(f"Failed to fetch commits for {repo_data['name']}: {e}")
                    self.errors.append(f"Commit fetch failed: {repo_data['name']}")

            return GitHubData(repositories=repos, commit_histories=commit_histories)

        return fetch_with_retry()

    def _generate_svgs(self, username: str, github_data: GitHubData) -> List[str]:
        """Generate SVG visualizations (FR-011: continue if fails)."""
        available_svgs = []
        svg_types = ["overview", "heatmap", "languages", "fun", "streaks", "release"]

        for svg_type in svg_types:
            try:
                svg_content = self._generate_single_svg(svg_type, username, github_data)
                svg_path = f"output/{svg_type}.svg"

                with open(svg_path, "w", encoding="utf-8") as f:
                    f.write(svg_content)

                available_svgs.append(svg_type)
                logger.info(f"âœ“ Generated {svg_type}.svg")

            except Exception as e:
                logger.warn(f"Failed to generate {svg_type}.svg: {e}")
                self.warnings.append(f"SVG generation failed: {svg_type}")
                # Continue to next SVG (FR-011: partial failures OK)

        return available_svgs

    def _analyze_repositories(
        self, username: str, repositories: List[Repository], commit_histories: Dict
    ) -> List[RepositoryAnalysis]:
        """Analyze repositories (FR-012: individual repo failures don't block report)."""
        analyses = []

        # Rank repositories
        ranked = self.ranker.rank_repositories(repositories, commit_histories, top_n=50)

        for rank, (repo, score) in enumerate(ranked, 1):
            try:
                # Generate summary (may use cache or fallback)
                summary = self.summarizer.summarize_repository(repo, readme_content, commit_histories.get(repo.name))

                # Analyze dependencies (may fail gracefully)
                tech_stack = None
                try:
                    tech_stack = self._analyze_dependencies(username, repo)
                except Exception as e:
                    logger.debug(f"Dependency analysis skipped for {repo.name}: {e}")

                analysis = RepositoryAnalysis(
                    repository=repo,
                    commit_history=commit_histories.get(repo.name),
                    summary=summary,
                    tech_stack=tech_stack,
                    rank=rank,
                    composite_score=score,
                )
                analyses.append(analysis)

            except Exception as e:
                logger.warn(f"Failed to analyze {repo.name}: {e}")
                self.errors.append(f"Repository analysis failed: {repo.name}")
                # Continue to next repository (FR-012: partial results OK)

        return analyses
```

**Error Handling Strategy**:

| Stage | Failure Impact | Action | Requirement |
|-------|---------------|--------|-------------|
| **GitHub API Fetch** | Critical | Fail entire workflow with clear error | Data dependency |
| **Individual Repo Fetch** | Non-critical | Log warning, continue with partial data | FR-012 |
| **SVG Generation** | Non-critical | Log warning, note missing SVG in report | FR-011 |
| **Repository Analysis** | Non-critical | Log warning, include successful analyses | FR-012 |
| **Report Generation** | Critical | Fail with error (should never happen if data exists) | Core feature |

**Data Sharing Strategy**:
- **Primary**: Disk cache (`.cache/` directory) - existing `APICache` class
- **Secondary**: In-memory objects passed between workflow stages
- **Benefits**: Cache survives retries, speeds up subsequent runs, reduces API calls

**Retry Logic Integration** (FR-016):
- Use `tenacity` library (already in `requirements.txt`)
- Exponential backoff: 1 min â†’ 5 min â†’ 15 min (3 attempts)
- Apply to: GitHub API calls, AI summarization (already exists in `summarizer.py`)
- Don't apply to: SVG generation, file I/O (fast operations that shouldn't need retry)

---

## Research Task 3: Report Structure Best Practices

**Context**: The report must work on GitHub profile pages, repository browsers, and local markdown viewers with proper rendering.

### Decision

**Use standard GitHub-Flavored Markdown (GFM) with relative-path SVG embedding** following GitHub's rendering conventions.

### Rationale

1. **Platform Compatibility**: GitHub supports standard markdown image syntax for SVG files with automatic sanitization ([Alex Chan - SVG Rendering](https://alexwlchan.net/til/2024/how-to-render-svgs-on-github/)). This works across github.com, GitHub Pages, and local viewers.

2. **Security & Performance**: GitHub automatically sanitizes SVGs and serves them with correct HTTP headers when using `![alt](path.svg)` syntax, removing security risks like embedded scripts.

3. **Existing Evidence**: Current dated reports (`markhazleton-analysis-20251230.md`) use pure markdown successfully - extending this pattern ensures consistency.

### Alternatives Considered

1. **HTML `<img>` tags** (rejected for primary use)
   - **Pros**: More explicit, supports additional attributes (width, height)
   - **Cons**: Less "markdown-native", harder to read in raw form
   - **When to use**: Only if need custom sizing (use standard markdown by default)

2. **Inline SVG code** (rejected)
   - **Pros**: No external file dependencies
   - **Cons**: GitHub strips `<svg>` tags entirely for security ([GitHub Community Discussion](https://github.com/orgs/community/discussions/151372)), bloats markdown file
   - **When to use**: Never for GitHub - only for self-hosted sites

3. **Base64-encoded data URIs** (rejected)
   - **Pros**: Self-contained
   - **Cons**: Massive markdown file size, unreadable, poor GitHub rendering performance

### Implementation Notes

**Recommended GFM Structure**:

```markdown
# GitHub Profile: markhazleton

**Generated**: 2025-12-30 12:00:00 UTC
**Report Version**: 1.0.0
**Repositories Analyzed**: 48

---

## Profile Overview

### Activity Dashboard

![Overview Statistics](../overview.svg)

### Commit Activity

![Commit Heatmap](../heatmap.svg)

![Coding Streaks](../streaks.svg)

### Technology Breakdown

![Language Distribution](../languages.svg)

![Fun Statistics](../fun.svg)

### Release Patterns

![Release Cadence](../release.svg)

---

## Top 50 Repositories

### #1. [git-spark](https://github.com/markhazleton/git-spark)

â­ 0 | ðŸ”± 0 | ðŸ“ TypeScript | ðŸ“Š 81 commits (90d)

ðŸ‘¥ 2 contributors | ðŸŒ 5 languages | ðŸ’¾ 1.3 MB | ðŸš€ 38.8 commits/month

**Quality**: âœ… CI/CD | âœ… Tests | âœ… License | âœ… Docs

**Releases**: 2 | Latest: 2025-11-20 (40 days ago)

Here's a concise technical summary for the git-spark repository:

Git Spark is a comprehensive Git repository analytics tool built with TypeScript...

**Technology Stack Currency**: âœ… 100/100
**Dependencies**: 19 total (19 current, 0 outdated)

**Created**: 2025-09-29 (92 days ago)
**Last Modified**: 2025-12-29 (yesterday)

---

### #2. [WebSpark.HttpClientUtility](https://github.com/markhazleton/WebSpark.HttpClientUtility)

â­ 0 | ðŸ”± 0 | ðŸ“ C# | ðŸ“Š 62 commits (90d)

...

---

## Report Metadata

- **Generation Time**: 343.4 seconds
- **Total API Calls**: 0 (cached)
- **Total AI Tokens**: 12,450
- **Success Rate**: 100.0%

*Generated by [Stats Spark](https://github.com/markhazleton/github-stats-spark)*
```

**SVG Embedding Patterns**:

```python
def _embed_svg(self, name: str, relative_path: str, fallback_text: str = None) -> str:
    """Generate SVG embedding with fallback.

    Args:
        name: Display name for alt text
        relative_path: Relative path from report location (e.g., "../overview.svg")
        fallback_text: Text to show if SVG missing (default: auto-generated)

    Returns:
        Markdown image syntax or fallback text
    """
    fallback = fallback_text or f"*{name} visualization unavailable*"

    # For unified report at /output/reports/username-analysis.md
    # SVGs are at /output/*.svg
    # Therefore relative path is ../*.svg

    return f"![{name}]({relative_path})"
```

**Table Formatting for Repository Analysis**:

The existing `ReportGenerator._format_repository_entry()` already uses optimal GFM patterns:
- **Emoji indicators**: â­ stars, ðŸ”± forks, ðŸ“ language, ðŸ“Š commits - universally supported
- **Inline stats**: Pipe-separated (`|`) for scanability
- **Quality badges**: âœ… emoji + text for accessibility
- **Structured paragraphs**: Blank lines between sections for readability

**Header/Footer Metadata Structure**:

```python
def _generate_header(self, report: UnifiedReport) -> str:
    """Generate header with metadata and navigation."""
    return f"""# GitHub Profile: {report.username}

**Generated**: {report.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}
**Report Version**: {report.version}
**Repositories Analyzed**: {report.total_repos}
**AI Summary Rate**: {report.ai_summary_rate:.1f}%

> ðŸ’¡ **Navigation**: [Profile Overview](#profile-overview) | [Top Repositories](#top-50-repositories) | [Metadata](#report-metadata)

---"""

def _generate_footer(self, report: UnifiedReport) -> str:
    """Generate footer with metadata and attribution."""
    error_section = ""
    if report.errors:
        error_section = f"""
## âš ï¸ Generation Warnings

{chr(10).join(f"- {error}" for error in report.errors[:5])}
{"..." if len(report.errors) > 5 else ""}

"""

    return f"""---

{error_section}
## Report Metadata

- **Generation Time**: {report.generation_time:.1f} seconds
- **SVGs Generated**: {len(report.available_svgs)}/6
- **Total API Calls**: {report.total_api_calls}
- **Total AI Tokens**: {report.total_ai_tokens:,}
- **Success Rate**: {report.success_rate:.1f}%

### Data Sources

- GitHub API (public repositories only)
- Anthropic Claude API (repository summaries)
- Dependency package registries (npm, PyPI, RubyGems, Go, Maven, NuGet)

### Report Details

- **Composite Score Weights**: Popularity 30% â€¢ Activity 45% â€¢ Health 25%
- **Technology Currency**: Calculated from latest versions in package registries
- **AI Model**: {report.ai_model or "N/A"}

---

*Generated by [Stats Spark](https://github.com/markhazleton/github-stats-spark)*
*Last updated: {report.timestamp.strftime('%Y-%m-%d')}*"""
```

**Cross-Platform Compatibility Checklist**:

| Feature | GitHub.com | GitHub Pages | VSCode Preview | Markdown Editors |
|---------|-----------|--------------|----------------|------------------|
| Relative SVG paths | âœ… | âœ… | âœ… | âœ… |
| Emoji indicators | âœ… | âœ… | âœ… | âš ï¸ (depends) |
| Anchor links | âœ… | âœ… | âœ… | âš ï¸ (depends) |
| Tables | âœ… | âœ… | âœ… | âœ… |
| Blockquotes | âœ… | âœ… | âœ… | âœ… |

**SEO & Accessibility Considerations**:

```python
def _embed_svg_accessible(self, name: str, path: str, description: str) -> str:
    """Generate accessible SVG embedding with descriptive alt text.

    Args:
        name: Short name for alt text
        path: Relative path to SVG
        description: Detailed description for screen readers

    Returns:
        Markdown with accessible alt text
    """
    # Alt text structure: "name: description"
    alt_text = f"{name}: {description}"
    return f"![{alt_text}]({path})"

# Example usage:
self._embed_svg_accessible(
    name="Overview Statistics",
    path="../overview.svg",
    description="Dashboard showing Spark Score of 75.5, 1234 total commits, with Python, JavaScript, and TypeScript as top languages"
)
```

---

## Research Task 4: Backward Compatibility Strategy

**Context**: Existing system generates dated reports (`{username}-analysis-20251230.md`). We need to add non-dated unified reports (`{username}-analysis.md`) without breaking existing functionality.

### Decision

**Dual-output system with separate CLI command** (`spark analyze --unified` flag) for explicit control during transition period.

### Rationale

1. **Explicit User Control**: Adding a `--unified` flag to existing `spark analyze` command lets users opt-in to new behavior while preserving existing dated reports. This follows the principle of least surprise.

2. **Safe Migration Path**: Both outputs can coexist in `/output/reports/` without filename collisions. Users can compare behavior before fully migrating.

3. **Workflow Flexibility**: GitHub Actions workflow can generate both reports during transition, then switch to unified-only once validated.

### Alternatives Considered

1. **New top-level command (`spark unified`)** (rejected)
   - **Pros**: Clear separation, no chance of breaking existing command
   - **Cons**: Adds command sprawl, splits related functionality, harder to discover
   - **When to use**: If unified report had fundamentally different input parameters

2. **Auto-detect mode (generate both always)** (rejected)
   - **Pros**: Seamless transition, no user action needed
   - **Cons**: Unexpected new files, potential confusion, wasted compute for unused reports
   - **When to use**: Never - users should explicitly choose behavior

3. **Replace dated reports entirely** (rejected)
   - **Pros**: Simplest long-term architecture
   - **Cons**: Breaks existing workflows immediately, loses historical reports
   - **When to use**: After proven transition period (6+ months)

### Implementation Notes

**Recommended CLI Extension**:

```python
# In src/spark/cli.py

def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(...)
    subparsers = parser.add_subparsers(dest="command")

    # Existing analyze command - EXTEND
    analyze_parser = subparsers.add_parser("analyze", help="Analyze repositories and generate report")
    analyze_parser.add_argument("--user", type=str, required=True)
    analyze_parser.add_argument("--output", type=str, default="output/reports")
    analyze_parser.add_argument("--top-n", type=int, default=50)

    # NEW: Add unified flag
    analyze_parser.add_argument(
        "--unified",
        action="store_true",
        help="Generate unified report (SVGs + analysis) instead of dated report",
    )

    # NEW: Add optional dated report in unified mode
    analyze_parser.add_argument(
        "--keep-dated",
        action="store_true",
        help="Also generate dated report when using --unified mode",
    )

    analyze_parser.add_argument("--config", type=str, default="config/spark.yml")
    analyze_parser.add_argument("--verbose", action="store_true")
    # ... rest of arguments

def handle_analyze(args, logger):
    """Handle analyze command - Generate repository analysis report."""
    logger.info("Stats Spark - Analyze Command")

    # Detect mode from arguments
    if args.unified:
        logger.info("Mode: Unified Report (SVGs + Analysis)")
        return handle_unified_analyze(args, logger)
    else:
        logger.info("Mode: Dated Report (Analysis Only)")
        return handle_dated_analyze(args, logger)

def handle_unified_analyze(args, logger):
    """Generate unified report with SVGs and analysis."""
    # ... [Implementation from Task 2 workflow] ...

    # Step 1: Execute unified workflow
    workflow = UnifiedReportWorkflow(config, cache)
    unified_report = workflow.execute(args.user)

    # Step 2: Generate unified markdown (non-dated)
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    unified_path = output_dir / f"{args.user}-analysis.md"

    unified_generator = UnifiedReportGenerator()
    unified_generator.generate_report(unified_report, str(unified_path))

    logger.info(f"âœ… Unified report: {unified_path}")

    # Step 3: Optionally generate dated report for comparison
    if args.keep_dated:
        dated_path = output_dir / f"{args.user}-analysis-{datetime.now().strftime('%Y%m%d')}.md"

        # Convert UnifiedReport to legacy Report format
        legacy_report = convert_to_legacy_format(unified_report)
        legacy_generator = ReportGenerator()
        legacy_generator.generate_report(legacy_report, str(dated_path))

        logger.info(f"âœ… Dated report: {dated_path}")

    return unified_report

def handle_dated_analyze(args, logger):
    """Generate dated report only (existing behavior)."""
    # ... [Existing implementation from lines 180-426 of cli.py] ...
    # NO CHANGES - preserve exact existing behavior
    pass
```

**File Naming Convention** (prevents collisions):

```
output/
â”œâ”€â”€ *.svg                                    # SVG files (non-dated, overwritten each run)
â””â”€â”€ reports/
    â”œâ”€â”€ {username}-analysis.md               # NEW: Unified report (non-dated, overwritten)
    â””â”€â”€ {username}-analysis-{YYYYMMDD}.md    # EXISTING: Dated report (preserved)
```

**Example file structure**:
```
output/
â”œâ”€â”€ overview.svg
â”œâ”€â”€ heatmap.svg
â”œâ”€â”€ languages.svg
â”œâ”€â”€ fun.svg
â”œâ”€â”€ streaks.svg
â”œâ”€â”€ release.svg
â””â”€â”€ reports/
    â”œâ”€â”€ markhazleton-analysis.md             # Always latest unified report
    â”œâ”€â”€ markhazleton-analysis-20251228.md    # Historical dated report
    â”œâ”€â”€ markhazleton-analysis-20251229.md    # Historical dated report
    â””â”€â”€ markhazleton-analysis-20251230.md    # Historical dated report
```

**GitHub Actions Workflow Update**:

```yaml
# .github/workflows/generate-stats.yml

name: Generate GitHub Statistics

on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sundays
  workflow_dispatch:
    inputs:
      report_mode:
        description: 'Report generation mode'
        required: false
        default: 'unified'
        type: choice
        options:
          - unified
          - dated
          - both

jobs:
  generate-stats:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Generate statistics (unified mode)
        if: ${{ github.event.inputs.report_mode == 'unified' || github.event.inputs.report_mode == '' }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python -m spark.cli analyze \
            --user markhazleton \
            --unified \
            --verbose

      - name: Generate statistics (dated mode - legacy)
        if: ${{ github.event.inputs.report_mode == 'dated' }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python -m spark.cli analyze \
            --user markhazleton \
            --verbose

      - name: Generate statistics (both modes)
        if: ${{ github.event.inputs.report_mode == 'both' }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python -m spark.cli analyze \
            --user markhazleton \
            --unified \
            --keep-dated \
            --verbose

      - name: Commit generated files
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add output/*.svg output/reports/*.md
          git diff --staged --quiet || git commit -m "Update GitHub statistics [skip ci]"

      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: ${{ github.ref }}
```

**Testing Strategy**:

```python
# tests/integration/test_backward_compatibility.py

import pytest
from pathlib import Path
from datetime import datetime

def test_unified_mode_generates_nondated_report(tmp_path):
    """Test that unified mode creates non-dated report."""
    # Run CLI: spark analyze --user testuser --unified --output tmp_path
    # Assert: {tmp_path}/testuser-analysis.md exists
    # Assert: No dated files created
    pass

def test_dated_mode_preserves_existing_behavior(tmp_path):
    """Test that dated mode still works unchanged."""
    # Run CLI: spark analyze --user testuser --output tmp_path
    # Assert: {tmp_path}/testuser-analysis-{YYYYMMDD}.md exists
    # Assert: No non-dated file created
    pass

def test_unified_with_keep_dated_creates_both(tmp_path):
    """Test that --unified --keep-dated creates both reports."""
    # Run CLI: spark analyze --user testuser --unified --keep-dated --output tmp_path
    # Assert: {tmp_path}/testuser-analysis.md exists
    # Assert: {tmp_path}/testuser-analysis-{YYYYMMDD}.md exists
    pass

def test_file_naming_collision_prevention(tmp_path):
    """Test that unified and dated reports don't conflict."""
    # Create existing dated report
    dated_path = tmp_path / f"testuser-analysis-{datetime.now().strftime('%Y%m%d')}.md"
    dated_path.write_text("Old dated content")

    # Run unified mode
    # Run CLI: spark analyze --user testuser --unified --output tmp_path

    # Assert: Unified report exists
    # Assert: Dated report unchanged
    unified_path = tmp_path / "testuser-analysis.md"
    assert unified_path.exists()
    assert dated_path.read_text() == "Old dated content"

def test_legacy_report_format_compatibility():
    """Test that dated reports maintain exact format."""
    # Generate dated report with known data
    # Compare output against golden file from previous version
    # Assert: Format identical (no breaking changes)
    pass
```

**Migration Timeline**:

| Phase | Duration | Action | Rollback Plan |
|-------|----------|--------|---------------|
| **Phase 0: Development** | 2 weeks | Implement unified report with `--unified` flag | N/A |
| **Phase 1: Parallel Testing** | 4 weeks | Generate both reports in workflow (`--unified --keep-dated`), compare outputs | Remove `--unified` flag |
| **Phase 2: Soft Launch** | 4 weeks | Document unified reports, encourage user adoption via README | Continue dated reports |
| **Phase 3: Default Switch** | - | Make `--unified` the default, add `--dated` flag for legacy mode | Revert default flag |
| **Phase 4: Deprecation** | 12+ weeks | Announce dated mode deprecation, provide migration guide | Extend support |
| **Phase 5: Removal** | - | Remove dated mode entirely, clean up legacy code | Impossible - breaking change |

**Configuration File Extension** (`config/spark.yml`):

```yaml
# Stats Spark Configuration

# Report Generation Settings
report:
  mode: unified  # Options: unified, dated, both
  keep_historical: true  # Keep dated reports when generating unified
  output_directory: output/reports

# Analyzer Settings (existing)
analyzer:
  top_repositories: 50
  ranking_weights:
    popularity: 0.30
    activity: 0.45
    health: 0.25

  # NEW: Unified report settings
  unified_report:
    include_svgs: true
    svg_order: [overview, heatmap, streaks, release, languages, fun]
    fallback_on_missing_svg: true
    include_footer_metadata: true

# Visualization Settings (existing)
visualization:
  theme: spark-dark
  # ...
```

---

## Summary & Next Steps

### Key Decisions

1. **Templating**: Use Python f-strings with modular helper methods (no Jinja2)
2. **Orchestration**: Sequential execution with continue-on-error pattern
3. **Markdown Structure**: Standard GFM with relative-path SVG embedding
4. **Compatibility**: Dual-output system with `--unified` CLI flag

### Implementation Priority

**Phase 1 (Immediate)**:
1. Create `UnifiedReportGenerator` class extending `ReportGenerator` pattern
2. Implement `UnifiedReportWorkflow` with sequential stages and error handling
3. Add `--unified` flag to `spark analyze` CLI command
4. Write integration tests for both modes

**Phase 2 (Follow-up)**:
1. Update GitHub Actions workflow to use `--unified` mode
2. Generate both reports in parallel for comparison period
3. Document migration path in user guide
4. Add configuration options to `spark.yml`

### Dependencies

- **No new dependencies required** (leverage existing: tenacity, PyGithub, svgwrite, anthropic)
- **Existing infrastructure reusable**: `APICache`, `ReportGenerator`, `RepositoryRanker`, `RepositorySummarizer`
- **Minor extensions needed**: CLI argument parsing, workflow orchestration class

### Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Breaking existing reports | Low | High | Parallel testing with `--keep-dated` flag |
| SVG path issues across viewers | Medium | Medium | Test on GitHub.com, VSCode, GitHub Pages |
| Partial failure handling bugs | Medium | Medium | Comprehensive error collection and reporting |
| User confusion about modes | Medium | Low | Clear CLI help text and documentation |

### References

- [Jinja 2 templates with Python - Phil Massyn](https://www.massyn.net/2024/11/17/jinja-2-templates-with-python)
- [Just into Data - Generate Reports with Python](https://www.justintodata.com/generate-reports-with-python/)
- [Engineering for Data Science - Python String Formatting](https://engineeringfordatascience.com/posts/python_string_formatting_for_data_science/)
- [Alex Chan - How to Render SVGs on GitHub](https://alexwlchan.net/til/2024/how-to-render-svgs-on-github/)
- [GitHub Gist - Linking to SVG files hosted on github](https://gist.github.com/ctalladen78/612b88460b00664af4fee8a34aab50e9)
- [Advanced Systems Consulting - Python Workflow Tools](https://www.advsyscon.com/blog/workload-orchestration-tools-python/)
- [Medium - Workflow Orchestration: Building Complex AI Pipelines](https://medium.com/@omark.k.aly/workflow-orchestration-building-complex-ai-pipelines-c8504ab8306f)

---

**Status**: âœ… Research complete - Ready for Phase 1 design
**Next Document**: `data-model.md` (UnifiedReport entity specification)
