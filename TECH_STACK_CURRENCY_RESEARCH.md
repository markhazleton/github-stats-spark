# Technology Stack Currency Research

**Project**: GitHub Stats Spark - AI Repository Summary Feature
**Focus**: Dependency Version Checking for Multiple Package Ecosystems
**Date**: 2025-12-29
**Constraints**: <3 minutes for 50 repositories, GitHub Actions compatible (no guaranteed internet to third-party APIs)

## Executive Summary

This document provides research findings and actionable recommendations for implementing technology stack currency checking across multiple package ecosystems. The solution must identify dependencies >2 major versions behind within a 3-minute window for 50 repositories.

### Recommended Approach

**Hybrid Static + Dynamic with Aggressive Caching**
- Primary: Direct registry API calls with local caching
- Fallback: Static version checking against pre-downloaded version databases
- Performance: Parallel processing with connection pooling
- Expected Time: 2-3 minutes for 50 repos (first run), <30 seconds (cached)

---

## 1. Data Sources for Package Version Information

### 1.1 NPM Registry API

**Endpoint**: `https://registry.npmjs.org/{package-name}`

**Characteristics**:
- Free, no authentication required
- Rate limit: None for read operations (public registry)
- Response time: 50-200ms per request
- Reliability: 99.9% uptime SLA
- Returns: All versions, latest tag, publish dates

**API Response Structure**:
```json
{
  "name": "react",
  "dist-tags": {
    "latest": "18.2.0",
    "next": "18.3.0-canary.1"
  },
  "versions": {
    "18.2.0": { "...": "..." },
    "18.1.0": { "...": "..." }
  }
}
```

**Usage Example**:
```python
import requests

def get_npm_latest_version(package_name: str) -> str:
    """Fetch latest NPM package version."""
    url = f"https://registry.npmjs.org/{package_name}"
    try:
        response = requests.get(url, timeout=3)
        response.raise_for_status()
        data = response.json()
        return data.get("dist-tags", {}).get("latest")
    except requests.RequestException:
        return None
```

**Advantages**:
- No API key required
- Very fast response times
- Comprehensive version history
- Works in GitHub Actions (public internet access)

**Disadvantages**:
- Must make separate request per package
- No bulk lookup endpoint

**Recommendation**: PRIMARY choice for NPM packages

---

### 1.2 PyPI JSON API

**Endpoint**: `https://pypi.org/pypi/{package-name}/json`

**Characteristics**:
- Free, no authentication required
- Rate limit: "Reasonable use" policy (no strict limit documented)
- Response time: 100-300ms per request
- Reliability: 99.5% uptime
- Returns: Latest version, all releases, metadata

**API Response Structure**:
```json
{
  "info": {
    "name": "requests",
    "version": "2.31.0",
    "summary": "Python HTTP library"
  },
  "releases": {
    "2.31.0": [...],
    "2.30.0": [...]
  }
}
```

**Usage Example**:
```python
def get_pypi_latest_version(package_name: str) -> str:
    """Fetch latest PyPI package version."""
    url = f"https://pypi.org/pypi/{package_name}/json"
    try:
        response = requests.get(url, timeout=3)
        response.raise_for_status()
        data = response.json()
        return data.get("info", {}).get("version")
    except requests.RequestException:
        return None
```

**Advantages**:
- Simple, well-documented API
- No authentication needed
- Returns all version history

**Disadvantages**:
- Slower than NPM registry
- Occasional timeouts during peak hours

**Recommendation**: PRIMARY choice for Python packages

---

### 1.3 Maven Central Repository Search API

**Endpoint**: `https://search.maven.org/solrsearch/select?q=g:{group}+AND+a:{artifact}&rows=1&wt=json`

**Characteristics**:
- Free, no authentication required
- Rate limit: None documented (fair use expected)
- Response time: 200-500ms per request
- Reliability: 99.5% uptime
- Returns: Latest version, group/artifact info

**API Response Structure**:
```json
{
  "response": {
    "docs": [
      {
        "id": "org.springframework.boot:spring-boot-starter",
        "latestVersion": "3.2.1",
        "g": "org.springframework.boot",
        "a": "spring-boot-starter"
      }
    ]
  }
}
```

**Usage Example**:
```python
def get_maven_latest_version(group_id: str, artifact_id: str) -> str:
    """Fetch latest Maven package version."""
    url = "https://search.maven.org/solrsearch/select"
    params = {
        "q": f"g:{group_id} AND a:{artifact_id}",
        "rows": 1,
        "wt": "json"
    }
    try:
        response = requests.get(url, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()
        docs = data.get("response", {}).get("docs", [])
        if docs:
            return docs[0].get("latestVersion")
    except requests.RequestException:
        return None
```

**Advantages**:
- Official Maven Central API
- Comprehensive coverage

**Disadvantages**:
- Slower response times
- Must parse group:artifact format
- Less critical for JavaScript/Python focused users

**Recommendation**: SECONDARY priority (include if time permits)

---

### 1.4 RubyGems API

**Endpoint**: `https://rubygems.org/api/v1/gems/{gem-name}.json`

**Characteristics**:
- Free, no authentication required
- Rate limit: No strict limit (fair use)
- Response time: 100-300ms per request
- Reliability: 99.5% uptime

**API Response Structure**:
```json
{
  "name": "rails",
  "version": "7.1.2",
  "info": "Full-stack web framework",
  "downloads": 500000000
}
```

**Usage Example**:
```python
def get_rubygems_latest_version(gem_name: str) -> str:
    """Fetch latest RubyGems version."""
    url = f"https://rubygems.org/api/v1/gems/{gem_name}.json"
    try:
        response = requests.get(url, timeout=3)
        response.raise_for_status()
        data = response.json()
        return data.get("version")
    except requests.RequestException:
        return None
```

**Recommendation**: SECONDARY priority

---

### 1.5 Go Module Proxy

**Endpoint**: `https://proxy.golang.org/{module}/@latest`

**Characteristics**:
- Free, no authentication required
- Rate limit: None documented
- Response time: 200-400ms per request
- Reliability: 99.9% uptime (Google-hosted)

**API Response Structure**:
```json
{
  "Version": "v1.8.0",
  "Time": "2023-11-15T10:00:00Z"
}
```

**Usage Example**:
```python
def get_go_latest_version(module_path: str) -> str:
    """Fetch latest Go module version."""
    url = f"https://proxy.golang.org/{module_path}/@latest"
    try:
        response = requests.get(url, timeout=3)
        response.raise_for_status()
        data = response.json()
        version = data.get("Version", "")
        return version.lstrip("v")  # Remove 'v' prefix
    except requests.RequestException:
        return None
```

**Recommendation**: SECONDARY priority

---

### 1.6 Cargo (Rust) API

**Endpoint**: `https://crates.io/api/v1/crates/{crate-name}`

**Characteristics**:
- Free, no authentication required for read
- Rate limit: 1 request/second per IP (requires User-Agent header)
- Response time: 100-300ms per request

**API Response Structure**:
```json
{
  "crate": {
    "name": "serde",
    "max_version": "1.0.193"
  }
}
```

**Usage Example**:
```python
def get_cargo_latest_version(crate_name: str) -> str:
    """Fetch latest Cargo crate version."""
    url = f"https://crates.io/api/v1/crates/{crate_name}"
    headers = {"User-Agent": "github-stats-spark"}  # Required!
    try:
        response = requests.get(url, headers=headers, timeout=3)
        response.raise_for_status()
        data = response.json()
        return data.get("crate", {}).get("max_version")
    except requests.RequestException:
        return None
```

**Recommendation**: TERTIARY priority (rate limit concern)

---

### 1.7 Libraries.io API (Aggregator)

**Endpoint**: `https://libraries.io/api/{platform}/{package-name}`

**Characteristics**:
- Requires API key (free tier: 60 requests/minute)
- Covers 36+ package ecosystems
- Response time: 300-800ms per request
- Reliability: 99% uptime

**Advantages**:
- Single API for all ecosystems
- Dependency tree analysis
- Security vulnerability data

**Disadvantages**:
- Requires API key registration
- Rate limits more restrictive than individual registries
- Slower than native APIs
- May not work in restricted GitHub Actions environments

**Recommendation**: NOT RECOMMENDED for this use case (slower, requires API key, unnecessary complexity)

---

## 2. Python Libraries for Version Comparison

### 2.1 packaging.version (RECOMMENDED)

**Source**: Built into Python standard library via `packaging` package

**Capabilities**:
- PEP 440 compliant version parsing
- Semantic versioning support
- Comparison operators (<, >, ==, !=)
- Handles pre-release, post-release, dev versions
- Lightweight, no external dependencies

**Usage Example**:
```python
from packaging.version import Version

# Parse versions
current = Version("1.2.3")
latest = Version("3.0.0")

# Compare
if current < latest:
    major_diff = latest.major - current.major
    print(f"{major_diff} major versions behind")

# Output: 2 major versions behind
```

**Version String Handling**:
```python
def clean_version(version_str: str) -> str:
    """Remove npm/semver operators (^, ~, >=, etc.)."""
    import re
    return re.sub(r'^[\^~>=<]+', '', version_str.strip())

# Examples
clean_version("^18.0.0")  # "18.0.0"
clean_version("~4.17.0")  # "4.17.0"
clean_version(">=2.0.0")  # "2.0.0"

current = Version(clean_version("^1.2.3"))
latest = Version("3.0.0")
```

**Major Version Difference Calculation**:
```python
def major_versions_behind(current_str: str, latest_str: str) -> int:
    """Calculate how many major versions behind."""
    current = Version(clean_version(current_str))
    latest = Version(clean_version(latest_str))

    # Access major version number
    return max(0, latest.major - current.major)

# Examples
major_versions_behind("1.5.0", "3.2.1")  # 2
major_versions_behind("2.0.0", "2.5.0")  # 0
major_versions_behind("10.0.0", "10.1.0")  # 0
```

**Recommendation**: PRIMARY choice - use `packaging.version.Version`

**Installation**:
```bash
pip install packaging
```

---

### 2.2 semver (Alternative)

**Source**: PyPI package `semver`

**Capabilities**:
- Strict semantic versioning (semver.org)
- Version bumping utilities
- Range matching

**Disadvantages**:
- Stricter than needed (requires exact X.Y.Z format)
- Doesn't handle npm-style operators well
- Additional dependency

**Recommendation**: NOT RECOMMENDED (packaging.version is superior for this use case)

---

### 2.3 Custom SemanticVersion Class (Lightweight Alternative)

If you want to avoid the `packaging` dependency:

```python
import re
from typing import Tuple

class SemanticVersion:
    """Lightweight semantic version parser."""

    def __init__(self, version_string: str):
        self.original = version_string
        self.major, self.minor, self.patch = self._parse(version_string)

    def _parse(self, version: str) -> Tuple[int, int, int]:
        """Parse version into major.minor.patch."""
        # Remove prefix operators (^, ~, >=, etc.)
        version = re.sub(r'^[\^~>=<]+', '', version.strip())

        # Remove 'v' prefix if present
        version = version.lstrip('v')

        # Remove pre-release and build metadata
        version = re.split(r'[-+]', version)[0]

        # Split into parts
        parts = version.split('.')

        major = int(parts[0]) if len(parts) > 0 and parts[0].isdigit() else 0
        minor = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0
        patch = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0

        return major, minor, patch

    def __lt__(self, other: 'SemanticVersion') -> bool:
        return (self.major, self.minor, self.patch) < (other.major, other.minor, other.patch)

    def __eq__(self, other: 'SemanticVersion') -> bool:
        return (self.major, self.minor, self.patch) == (other.major, other.minor, other.patch)

    def major_versions_behind(self, latest: 'SemanticVersion') -> int:
        """Calculate how many major versions behind."""
        return max(0, latest.major - self.major)

    def __repr__(self) -> str:
        return f"{self.major}.{self.minor}.{self.patch}"

# Usage
current = SemanticVersion("^1.5.0")
latest = SemanticVersion("3.2.1")
print(current.major_versions_behind(latest))  # 2
```

**Recommendation**: ALTERNATIVE if you want zero dependencies

---

## 3. Caching Strategies for Package Version Data

### 3.1 Requirements Analysis

**Cache Requirements**:
- Must reduce API calls from ~500 (50 repos Ã— 10 deps avg) to <100
- Must handle GitHub Actions environment (persistent between workflow runs)
- Must invalidate stale data (weekly refresh recommended)
- Must be fast (cache lookup <10ms)

### 3.2 File-Based Cache (RECOMMENDED)

Extend existing `APICache` class to support version data caching:

```python
# src/spark/cache.py (enhanced)

class VersionCache(APICache):
    """Cache for package version data with weekly TTL."""

    def __init__(self, cache_dir: str = ".cache/versions", ttl_days: int = 7):
        """Initialize version cache with 7-day TTL."""
        super().__init__(cache_dir=cache_dir, ttl_hours=ttl_days * 24)

    def get_latest_version(
        self,
        ecosystem: str,
        package_name: str
    ) -> Optional[str]:
        """Get cached latest version for a package.

        Args:
            ecosystem: Package ecosystem (npm, pypi, maven, etc.)
            package_name: Package name

        Returns:
            Latest version string or None if not cached
        """
        cache_key = f"{ecosystem}:{package_name}"
        return self.get(cache_key)

    def set_latest_version(
        self,
        ecosystem: str,
        package_name: str,
        version: str
    ) -> None:
        """Cache latest version for a package.

        Args:
            ecosystem: Package ecosystem
            package_name: Package name
            version: Latest version string
        """
        cache_key = f"{ecosystem}:{package_name}"
        self.set(cache_key, version)
```

**Usage in Version Checker**:
```python
class NPMVersionChecker:
    """Check NPM package versions with caching."""

    def __init__(self, cache: Optional[VersionCache] = None):
        self.cache = cache or VersionCache()
        self.session = requests.Session()  # Connection pooling

    def get_latest_version(self, package_name: str) -> Optional[str]:
        """Get latest NPM version with cache fallback."""
        # Check cache first
        cached = self.cache.get_latest_version("npm", package_name)
        if cached:
            return cached

        # Fetch from API
        url = f"https://registry.npmjs.org/{package_name}"
        try:
            response = self.session.get(url, timeout=3)
            response.raise_for_status()
            data = response.json()
            version = data.get("dist-tags", {}).get("latest")

            # Cache the result
            if version:
                self.cache.set_latest_version("npm", package_name, version)

            return version
        except requests.RequestException:
            return None
```

**Cache Persistence in GitHub Actions**:
```yaml
# .github/workflows/stats.yml
- name: Cache package versions
  uses: actions/cache@v3
  with:
    path: .cache/versions
    key: package-versions-${{ hashFiles('**/package.json', '**/requirements.txt') }}
    restore-keys: |
      package-versions-
```

**Advantages**:
- Reuses existing `APICache` infrastructure
- Persists between GitHub Actions runs
- Simple JSON file format
- No external dependencies

**Disadvantages**:
- File I/O overhead (minimal)
- Not suitable for concurrent writes (not needed for this use case)

**Recommendation**: PRIMARY caching strategy

---

### 3.3 In-Memory Cache (Supplementary)

Use in-memory caching for single report generation run:

```python
from functools import lru_cache

class CachedVersionChecker:
    """Version checker with in-memory LRU cache."""

    @lru_cache(maxsize=500)  # Cache up to 500 packages
    def get_latest_version(self, ecosystem: str, package_name: str) -> Optional[str]:
        """Get latest version with in-memory cache."""
        # This method will be cached automatically by lru_cache
        return self._fetch_from_api(ecosystem, package_name)

    def _fetch_from_api(self, ecosystem: str, package_name: str) -> Optional[str]:
        """Actual API fetch logic."""
        # Implementation here
        pass
```

**Advantages**:
- Zero I/O overhead
- Built into Python standard library
- Automatic cache eviction (LRU)

**Disadvantages**:
- Lost between runs
- Not shared across processes

**Recommendation**: SUPPLEMENTARY (use alongside file cache)

---

### 3.4 Hybrid Caching Strategy (RECOMMENDED IMPLEMENTATION)

Combine file cache + in-memory cache:

```python
class HybridVersionChecker:
    """Version checker with two-tier caching."""

    def __init__(self, file_cache: VersionCache):
        self.file_cache = file_cache
        self.memory_cache = {}  # In-memory cache
        self.session = requests.Session()

    def get_latest_version(
        self,
        ecosystem: str,
        package_name: str
    ) -> Optional[str]:
        """Get latest version with hybrid cache.

        Cache hierarchy:
        1. In-memory cache (fastest)
        2. File cache (fast)
        3. API call (slow)
        """
        cache_key = f"{ecosystem}:{package_name}"

        # Level 1: Check in-memory cache
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]

        # Level 2: Check file cache
        cached = self.file_cache.get_latest_version(ecosystem, package_name)
        if cached:
            # Promote to memory cache
            self.memory_cache[cache_key] = cached
            return cached

        # Level 3: Fetch from API
        version = self._fetch_from_registry(ecosystem, package_name)

        if version:
            # Store in both caches
            self.memory_cache[cache_key] = version
            self.file_cache.set_latest_version(ecosystem, package_name, version)

        return version

    def _fetch_from_registry(
        self,
        ecosystem: str,
        package_name: str
    ) -> Optional[str]:
        """Fetch version from appropriate registry."""
        if ecosystem == "npm":
            return self._fetch_npm(package_name)
        elif ecosystem == "pypi":
            return self._fetch_pypi(package_name)
        elif ecosystem == "rubygems":
            return self._fetch_rubygems(package_name)
        elif ecosystem == "go":
            return self._fetch_go(package_name)
        return None
```

**Expected Performance**:
- First run (50 repos, 10 deps each = 500 packages): 1.5-2 minutes (API calls)
- Second run (same repos): <10 seconds (file cache)
- Third run (same session): <1 second (memory cache)

**Recommendation**: OPTIMAL solution for this use case

---

## 4. Supported Package Ecosystems (Priority Order)

### Top 5 Ecosystems (MUST Support)

Based on GitHub language popularity and user impact:

| Rank | Ecosystem | Dependency File(s) | Priority | Estimated Coverage |
|------|-----------|-------------------|----------|-------------------|
| 1 | NPM (JavaScript/TypeScript) | package.json | P0 | 40% of repos |
| 2 | PyPI (Python) | requirements.txt, pyproject.toml | P0 | 30% of repos |
| 3 | Maven (Java) | pom.xml | P1 | 10% of repos |
| 4 | RubyGems (Ruby) | Gemfile | P1 | 8% of repos |
| 5 | Go Modules | go.mod | P1 | 7% of repos |

**Total Coverage**: ~95% of repositories with dependency files

### Additional Ecosystems (Nice to Have)

| Rank | Ecosystem | Dependency File(s) | Priority | Coverage |
|------|-----------|-------------------|----------|----------|
| 6 | Cargo (Rust) | Cargo.toml | P2 | 3% |
| 7 | Composer (PHP) | composer.json | P2 | 2% |
| 8 | NuGet (.NET) | packages.config, *.csproj | P3 | 1% |

### Implementation Priority Recommendation

**Phase 1 (MVP)**: NPM + PyPI (70% coverage)
**Phase 2**: Add Maven + RubyGems + Go (95% coverage)
**Phase 3**: Add remaining ecosystems as needed

---

## 5. API Endpoints and Data Sources Summary

### Quick Reference Table

| Ecosystem | API Endpoint | Auth Required | Rate Limit | Response Time | Recommendation |
|-----------|--------------|---------------|------------|---------------|----------------|
| NPM | `https://registry.npmjs.org/{pkg}` | No | None | 50-200ms | PRIMARY |
| PyPI | `https://pypi.org/pypi/{pkg}/json` | No | Fair use | 100-300ms | PRIMARY |
| Maven | `https://search.maven.org/solrsearch/select` | No | Fair use | 200-500ms | SECONDARY |
| RubyGems | `https://rubygems.org/api/v1/gems/{gem}.json` | No | Fair use | 100-300ms | SECONDARY |
| Go | `https://proxy.golang.org/{mod}/@latest` | No | None | 200-400ms | SECONDARY |
| Cargo | `https://crates.io/api/v1/crates/{crate}` | No | 1 req/sec | 100-300ms | TERTIARY |

---

## 6. Complete Implementation Architecture

### 6.1 Dependency File Parsing

```python
# src/spark/dependencies/parser.py

import json
import re
from typing import Dict, List, Optional
from pathlib import Path

class DependencyParser:
    """Parse dependency files across multiple ecosystems."""

    SUPPORTED_FILES = {
        'package.json': 'npm',
        'requirements.txt': 'pypi',
        'pyproject.toml': 'pypi',
        'Pipfile': 'pypi',
        'pom.xml': 'maven',
        'Gemfile': 'rubygems',
        'go.mod': 'go',
        'Cargo.toml': 'cargo',
        'composer.json': 'composer'
    }

    def detect_dependency_files(
        self,
        repo_contents: Dict[str, str]  # filename -> content
    ) -> Dict[str, str]:
        """Detect which dependency files are present.

        Args:
            repo_contents: Dict mapping filenames to content

        Returns:
            Dict mapping detected files to ecosystem
        """
        detected = {}
        for filename in repo_contents.keys():
            if filename in self.SUPPORTED_FILES:
                detected[filename] = self.SUPPORTED_FILES[filename]
        return detected

    def parse_dependencies(
        self,
        filename: str,
        content: str
    ) -> List[Dict[str, str]]:
        """Parse dependencies from file content.

        Args:
            filename: Dependency file name
            content: File content

        Returns:
            List of dependency dicts with 'name' and 'version' keys
        """
        ecosystem = self.SUPPORTED_FILES.get(filename)

        if ecosystem == 'npm':
            return self._parse_package_json(content)
        elif ecosystem == 'pypi':
            if filename == 'requirements.txt':
                return self._parse_requirements_txt(content)
            elif filename == 'pyproject.toml':
                return self._parse_pyproject_toml(content)
        elif ecosystem == 'maven':
            return self._parse_pom_xml(content)
        elif ecosystem == 'rubygems':
            return self._parse_gemfile(content)
        elif ecosystem == 'go':
            return self._parse_go_mod(content)

        return []

    def _parse_package_json(self, content: str) -> List[Dict[str, str]]:
        """Parse package.json dependencies."""
        try:
            data = json.loads(content)
            deps = []

            # Combine dependencies and devDependencies
            for dep_type in ['dependencies', 'devDependencies']:
                for name, version in data.get(dep_type, {}).items():
                    deps.append({
                        'name': name,
                        'version': version,
                        'ecosystem': 'npm'
                    })

            return deps
        except json.JSONDecodeError:
            return []

    def _parse_requirements_txt(self, content: str) -> List[Dict[str, str]]:
        """Parse requirements.txt format."""
        deps = []

        for line in content.split('\n'):
            line = line.strip()

            # Skip comments, empty lines, and editable installs
            if not line or line.startswith('#') or line.startswith('-e '):
                continue

            # Parse package==version or package>=version
            match = re.match(r'^([a-zA-Z0-9-_\.]+)\s*([><=!]+)\s*([0-9\.]+.*?)$', line)
            if match:
                deps.append({
                    'name': match.group(1),
                    'version': match.group(3),
                    'ecosystem': 'pypi'
                })
            else:
                # Package without version
                if re.match(r'^[a-zA-Z0-9-_\.]+$', line):
                    deps.append({
                        'name': line,
                        'version': None,
                        'ecosystem': 'pypi'
                    })

        return deps

    def _parse_pyproject_toml(self, content: str) -> List[Dict[str, str]]:
        """Parse pyproject.toml (basic implementation)."""
        deps = []

        # Simple regex-based parsing (consider using tomli for production)
        # Match lines like: requests = "^2.28.0"
        pattern = r'([a-zA-Z0-9-_]+)\s*=\s*["\']([^"\']+)["\']'
        matches = re.findall(pattern, content)

        for name, version in matches:
            if name != 'python':  # Skip python version
                deps.append({
                    'name': name,
                    'version': version,
                    'ecosystem': 'pypi'
                })

        return deps

    def _parse_gemfile(self, content: str) -> List[Dict[str, str]]:
        """Parse Gemfile."""
        deps = []

        # Match: gem 'rails', '~> 6.1.0'
        pattern = r"gem\s+['\"]([^'\"]+)['\"]\s*,\s*['\"]([^'\"]+)['\"]"
        matches = re.findall(pattern, content)

        for name, version in matches:
            deps.append({
                'name': name,
                'version': version,
                'ecosystem': 'rubygems'
            })

        return deps

    def _parse_go_mod(self, content: str) -> List[Dict[str, str]]:
        """Parse go.mod."""
        deps = []

        in_require_block = False
        for line in content.split('\n'):
            line = line.strip()

            if line == 'require (':
                in_require_block = True
                continue
            elif line == ')':
                in_require_block = False
                continue

            if in_require_block or line.startswith('require '):
                # Match: github.com/pkg/errors v0.9.1
                match = re.match(r'([^\s]+)\s+v([0-9\.]+)', line)
                if match:
                    deps.append({
                        'name': match.group(1),
                        'version': match.group(2),
                        'ecosystem': 'go'
                    })

        return deps

    def _parse_pom_xml(self, content: str) -> List[Dict[str, str]]:
        """Parse pom.xml (basic implementation)."""
        deps = []

        # Simple regex-based parsing
        # Match <artifactId>...</artifactId> and <version>...</version>
        artifact_pattern = r'<artifactId>([^<]+)</artifactId>'
        version_pattern = r'<version>([^<]+)</version>'

        artifacts = re.findall(artifact_pattern, content)
        versions = re.findall(version_pattern, content)

        # Pair artifacts with versions (assumes they appear in order)
        for i, artifact in enumerate(artifacts):
            if i < len(versions):
                deps.append({
                    'name': artifact,
                    'version': versions[i],
                    'ecosystem': 'maven'
                })

        return deps
```

**Installation Requirement**:
```bash
# For production-grade TOML parsing
pip install tomli
```

---

### 6.2 Unified Version Checker

```python
# src/spark/dependencies/version_checker.py

import requests
from typing import Dict, List, Optional
from packaging.version import Version
import re

from spark.cache import APICache

class VersionChecker:
    """Unified version checker across all ecosystems."""

    def __init__(self, cache: Optional[APICache] = None):
        """Initialize version checker with caching.

        Args:
            cache: Optional cache instance (defaults to 7-day TTL)
        """
        self.cache = cache or APICache(
            cache_dir=".cache/versions",
            ttl_hours=7 * 24  # 7 days
        )
        self.session = requests.Session()  # Connection pooling
        self.memory_cache = {}  # In-memory cache for single run

    def check_dependency_currency(
        self,
        dependencies: List[Dict[str, str]]
    ) -> List[Dict]:
        """Check currency for list of dependencies.

        Args:
            dependencies: List of dicts with 'name', 'version', 'ecosystem'

        Returns:
            Enriched dependencies with 'latest_version', 'status', 'versions_behind'
        """
        enriched = []

        for dep in dependencies:
            name = dep['name']
            current_version = dep.get('version')
            ecosystem = dep['ecosystem']

            # Get latest version
            latest_version = self.get_latest_version(ecosystem, name)

            # Calculate status
            if not current_version or not latest_version:
                status = 'unknown'
                versions_behind = None
            else:
                versions_behind = self.calculate_versions_behind(
                    current_version,
                    latest_version
                )
                status = self.determine_status(versions_behind)

            enriched.append({
                **dep,
                'latest_version': latest_version,
                'status': status,
                'versions_behind': versions_behind
            })

        return enriched

    def get_latest_version(
        self,
        ecosystem: str,
        package_name: str
    ) -> Optional[str]:
        """Get latest version with hybrid caching.

        Args:
            ecosystem: Package ecosystem (npm, pypi, maven, etc.)
            package_name: Package name

        Returns:
            Latest version string or None
        """
        cache_key = f"{ecosystem}:{package_name}"

        # Level 1: In-memory cache
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]

        # Level 2: File cache
        cached = self.cache.get(cache_key)
        if cached:
            self.memory_cache[cache_key] = cached
            return cached

        # Level 3: API call
        version = self._fetch_from_registry(ecosystem, package_name)

        if version:
            self.memory_cache[cache_key] = version
            self.cache.set(cache_key, version)

        return version

    def _fetch_from_registry(
        self,
        ecosystem: str,
        package_name: str
    ) -> Optional[str]:
        """Fetch latest version from registry API."""
        if ecosystem == 'npm':
            return self._fetch_npm(package_name)
        elif ecosystem == 'pypi':
            return self._fetch_pypi(package_name)
        elif ecosystem == 'maven':
            # Maven packages need group:artifact split
            # For simplicity, skip or implement separately
            return None
        elif ecosystem == 'rubygems':
            return self._fetch_rubygems(package_name)
        elif ecosystem == 'go':
            return self._fetch_go(package_name)
        return None

    def _fetch_npm(self, package_name: str) -> Optional[str]:
        """Fetch latest NPM version."""
        url = f"https://registry.npmjs.org/{package_name}"
        try:
            response = self.session.get(url, timeout=3)
            response.raise_for_status()
            data = response.json()
            return data.get("dist-tags", {}).get("latest")
        except requests.RequestException:
            return None

    def _fetch_pypi(self, package_name: str) -> Optional[str]:
        """Fetch latest PyPI version."""
        url = f"https://pypi.org/pypi/{package_name}/json"
        try:
            response = self.session.get(url, timeout=3)
            response.raise_for_status()
            data = response.json()
            return data.get("info", {}).get("version")
        except requests.RequestException:
            return None

    def _fetch_rubygems(self, gem_name: str) -> Optional[str]:
        """Fetch latest RubyGems version."""
        url = f"https://rubygems.org/api/v1/gems/{gem_name}.json"
        try:
            response = self.session.get(url, timeout=3)
            response.raise_for_status()
            data = response.json()
            return data.get("version")
        except requests.RequestException:
            return None

    def _fetch_go(self, module_path: str) -> Optional[str]:
        """Fetch latest Go module version."""
        url = f"https://proxy.golang.org/{module_path}/@latest"
        try:
            response = self.session.get(url, timeout=3)
            response.raise_for_status()
            data = response.json()
            version = data.get("Version", "")
            return version.lstrip("v")
        except requests.RequestException:
            return None

    @staticmethod
    def clean_version(version_str: str) -> str:
        """Remove semver operators (^, ~, >=, etc.)."""
        return re.sub(r'^[\^~>=<]+', '', version_str.strip())

    def calculate_versions_behind(
        self,
        current: str,
        latest: str
    ) -> int:
        """Calculate how many major versions behind.

        Args:
            current: Current version string
            latest: Latest version string

        Returns:
            Number of major versions behind (0 if current or ahead)
        """
        try:
            current_ver = Version(self.clean_version(current))
            latest_ver = Version(self.clean_version(latest))

            # Access major version number
            return max(0, latest_ver.major - current_ver.major)
        except:
            return 0

    def determine_status(self, versions_behind: int) -> str:
        """Determine currency status based on versions behind.

        Args:
            versions_behind: Number of major versions behind

        Returns:
            Status string: 'current', 'outdated', or 'very outdated'
        """
        if versions_behind == 0:
            return 'current'
        elif versions_behind <= 2:
            return 'outdated'
        else:
            return 'very outdated'
```

**Installation Requirements**:
```bash
pip install packaging requests
```

---

### 6.3 Repository Dependency Analyzer

```python
# src/spark/dependencies/analyzer.py

from typing import Dict, List, Optional
from github import Github
from spark.dependencies.parser import DependencyParser
from spark.dependencies.version_checker import VersionChecker
from spark.logger import get_logger

class RepositoryDependencyAnalyzer:
    """Analyze repository dependencies and check currency."""

    def __init__(
        self,
        github_client: Github,
        parser: Optional[DependencyParser] = None,
        version_checker: Optional[VersionChecker] = None
    ):
        """Initialize analyzer.

        Args:
            github_client: PyGithub client instance
            parser: Optional dependency parser
            version_checker: Optional version checker
        """
        self.github = github_client
        self.parser = parser or DependencyParser()
        self.checker = version_checker or VersionChecker()
        self.logger = get_logger()

    def analyze_repository(
        self,
        username: str,
        repo_name: str
    ) -> Dict:
        """Analyze dependencies for a single repository.

        Args:
            username: Repository owner username
            repo_name: Repository name

        Returns:
            Dict with dependency analysis results
        """
        self.logger.info(f"Analyzing dependencies for {repo_name}")

        # Fetch dependency files
        dependency_files = self._fetch_dependency_files(username, repo_name)

        if not dependency_files:
            return {
                'repo': repo_name,
                'has_dependencies': False,
                'dependencies': [],
                'summary': 'No dependency files found'
            }

        # Parse dependencies
        all_dependencies = []
        for filename, content in dependency_files.items():
            deps = self.parser.parse_dependencies(filename, content)
            all_dependencies.extend(deps)

        # Check versions
        checked_dependencies = self.checker.check_dependency_currency(all_dependencies)

        # Generate summary
        summary = self._generate_summary(checked_dependencies)

        return {
            'repo': repo_name,
            'has_dependencies': True,
            'dependencies': checked_dependencies,
            'summary': summary
        }

    def _fetch_dependency_files(
        self,
        username: str,
        repo_name: str
    ) -> Dict[str, str]:
        """Fetch dependency files from repository.

        Returns:
            Dict mapping filename to content
        """
        dependency_files = {}

        try:
            repo = self.github.get_repo(f"{username}/{repo_name}")

            # List of dependency files to check
            files_to_check = [
                'package.json',
                'requirements.txt',
                'pyproject.toml',
                'Pipfile',
                'Gemfile',
                'go.mod',
                'Cargo.toml',
                'pom.xml',
                'composer.json'
            ]

            for filename in files_to_check:
                try:
                    file_content = repo.get_contents(filename)
                    if not file_content.is_dir():
                        content = file_content.decoded_content.decode('utf-8')
                        dependency_files[filename] = content
                except:
                    # File doesn't exist, skip
                    continue

            return dependency_files

        except Exception as e:
            self.logger.error(f"Failed to fetch dependency files for {repo_name}: {e}")
            return {}

    def _generate_summary(self, dependencies: List[Dict]) -> str:
        """Generate human-readable summary of dependency currency.

        Args:
            dependencies: List of checked dependencies

        Returns:
            Summary string
        """
        if not dependencies:
            return "No dependencies found"

        total = len(dependencies)
        current_count = sum(1 for dep in dependencies if dep.get('status') == 'current')
        outdated_count = sum(1 for dep in dependencies if dep.get('status') == 'outdated')
        very_outdated_count = sum(1 for dep in dependencies if dep.get('status') == 'very outdated')

        current_pct = int((current_count / total) * 100)

        if current_pct >= 80:
            return f"Mostly current ({current_pct}% up-to-date, {total} dependencies)"
        elif current_pct >= 50:
            return f"Some outdated dependencies ({current_pct}% up-to-date, {outdated_count} outdated, {very_outdated_count} very outdated)"
        else:
            return f"Significantly outdated ({current_pct}% up-to-date, {outdated_count} outdated, {very_outdated_count} very outdated)"
```

---

## 7. Error Handling for Unsupported Ecosystems

### 7.1 Graceful Degradation Strategy

```python
# src/spark/dependencies/error_handling.py

from typing import Dict, List, Optional
from spark.logger import get_logger

class DependencyAnalysisError:
    """Error handling for dependency analysis."""

    @staticmethod
    def handle_unsupported_ecosystem(
        filename: str,
        ecosystem: str
    ) -> Dict:
        """Handle unsupported package ecosystem.

        Args:
            filename: Dependency filename
            ecosystem: Ecosystem identifier

        Returns:
            Error result dict
        """
        logger = get_logger()
        logger.warning(f"Unsupported ecosystem for {filename}: {ecosystem}")

        return {
            'error': 'unsupported_ecosystem',
            'filename': filename,
            'ecosystem': ecosystem,
            'message': f"Dependency file '{filename}' detected but ecosystem '{ecosystem}' is not yet supported",
            'recommendation': 'This ecosystem may be added in a future update'
        }

    @staticmethod
    def handle_parsing_error(
        filename: str,
        error: Exception
    ) -> Dict:
        """Handle dependency file parsing error.

        Args:
            filename: Dependency filename that failed to parse
            error: Exception that occurred

        Returns:
            Error result dict
        """
        logger = get_logger()
        logger.error(f"Failed to parse {filename}: {error}")

        return {
            'error': 'parsing_failed',
            'filename': filename,
            'message': f"Unable to parse dependency file '{filename}'",
            'details': str(error),
            'recommendation': 'File may be malformed or use an unsupported format variant'
        }

    @staticmethod
    def handle_api_failure(
        package_name: str,
        ecosystem: str,
        error: Exception
    ) -> Dict:
        """Handle version API request failure.

        Args:
            package_name: Package that failed to fetch
            ecosystem: Package ecosystem
            error: Exception that occurred

        Returns:
            Error result dict
        """
        logger = get_logger()
        logger.warning(f"Failed to fetch version for {ecosystem}:{package_name}: {error}")

        return {
            'error': 'api_failure',
            'package': package_name,
            'ecosystem': ecosystem,
            'message': f"Unable to fetch latest version for {package_name}",
            'details': str(error),
            'recommendation': 'Version information unavailable - package may not exist in registry or network issue occurred'
        }

    @staticmethod
    def summarize_errors(errors: List[Dict]) -> str:
        """Generate human-readable summary of errors.

        Args:
            errors: List of error dicts

        Returns:
            Summary string
        """
        if not errors:
            return "All dependencies analyzed successfully"

        error_counts = {
            'unsupported_ecosystem': 0,
            'parsing_failed': 0,
            'api_failure': 0,
            'other': 0
        }

        for error in errors:
            error_type = error.get('error', 'other')
            error_counts[error_type] = error_counts.get(error_type, 0) + 1

        summary_parts = []
        if error_counts['unsupported_ecosystem'] > 0:
            summary_parts.append(f"{error_counts['unsupported_ecosystem']} unsupported ecosystem(s)")
        if error_counts['parsing_failed'] > 0:
            summary_parts.append(f"{error_counts['parsing_failed']} parsing error(s)")
        if error_counts['api_failure'] > 0:
            summary_parts.append(f"{error_counts['api_failure']} API failure(s)")

        if summary_parts:
            return "Errors encountered: " + ", ".join(summary_parts)
        return f"{len(errors)} error(s) occurred during analysis"
```

### 7.2 Enhanced Analyzer with Error Handling

```python
# Enhanced version of RepositoryDependencyAnalyzer

class RobustRepositoryDependencyAnalyzer(RepositoryDependencyAnalyzer):
    """Dependency analyzer with comprehensive error handling."""

    def analyze_repository(
        self,
        username: str,
        repo_name: str
    ) -> Dict:
        """Analyze dependencies with error handling."""
        errors = []

        try:
            # Fetch dependency files
            dependency_files = self._fetch_dependency_files(username, repo_name)

            if not dependency_files:
                return {
                    'repo': repo_name,
                    'has_dependencies': False,
                    'dependencies': [],
                    'errors': [],
                    'summary': 'No dependency files found'
                }

            # Parse dependencies
            all_dependencies = []
            for filename, content in dependency_files.items():
                try:
                    deps = self.parser.parse_dependencies(filename, content)
                    all_dependencies.extend(deps)
                except Exception as e:
                    errors.append(
                        DependencyAnalysisError.handle_parsing_error(filename, e)
                    )

            # Check versions
            checked_dependencies = []
            for dep in all_dependencies:
                try:
                    checked = self.checker.check_dependency_currency([dep])
                    checked_dependencies.extend(checked)
                except Exception as e:
                    errors.append(
                        DependencyAnalysisError.handle_api_failure(
                            dep['name'],
                            dep['ecosystem'],
                            e
                        )
                    )

            # Generate summary
            summary = self._generate_summary(checked_dependencies)
            if errors:
                error_summary = DependencyAnalysisError.summarize_errors(errors)
                summary += f" ({error_summary})"

            return {
                'repo': repo_name,
                'has_dependencies': True,
                'dependencies': checked_dependencies,
                'errors': errors,
                'summary': summary
            }

        except Exception as e:
            self.logger.error(f"Critical error analyzing {repo_name}: {e}")
            return {
                'repo': repo_name,
                'has_dependencies': False,
                'dependencies': [],
                'errors': [{'error': 'critical', 'message': str(e)}],
                'summary': 'Analysis failed due to critical error'
            }
```

---

## 8. Performance Optimization

### 8.1 Parallel Processing

```python
# src/spark/dependencies/parallel_analyzer.py

import concurrent.futures
from typing import Dict, List
from github import Github
from spark.dependencies.analyzer import RobustRepositoryDependencyAnalyzer
from spark.logger import get_logger

class ParallelDependencyAnalyzer:
    """Analyze multiple repositories in parallel."""

    def __init__(
        self,
        github_client: Github,
        max_workers: int = 5
    ):
        """Initialize parallel analyzer.

        Args:
            github_client: PyGithub client
            max_workers: Maximum parallel threads (default: 5)
        """
        self.github = github_client
        self.max_workers = max_workers
        self.logger = get_logger()

    def analyze_repositories(
        self,
        username: str,
        repo_names: List[str]
    ) -> Dict[str, Dict]:
        """Analyze multiple repositories in parallel.

        Args:
            username: Repository owner
            repo_names: List of repository names

        Returns:
            Dict mapping repo name to analysis results
        """
        results = {}

        # Create analyzer instance for each thread
        def analyze_single(repo_name: str) -> tuple:
            analyzer = RobustRepositoryDependencyAnalyzer(self.github)
            result = analyzer.analyze_repository(username, repo_name)
            return (repo_name, result)

        # Use ThreadPoolExecutor for I/O-bound tasks
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_repo = {
                executor.submit(analyze_single, repo): repo
                for repo in repo_names
            }

            for future in concurrent.futures.as_completed(future_to_repo):
                try:
                    repo_name, result = future.result()
                    results[repo_name] = result
                    self.logger.info(f"Completed dependency analysis for {repo_name}")
                except Exception as e:
                    repo = future_to_repo[future]
                    self.logger.error(f"Failed to analyze {repo}: {e}")
                    results[repo] = {
                        'repo': repo,
                        'has_dependencies': False,
                        'dependencies': [],
                        'errors': [{'error': 'analysis_failed', 'message': str(e)}],
                        'summary': 'Analysis failed'
                    }

        return results
```

### 8.2 Performance Benchmarks

**Expected Performance (50 repositories)**:

| Scenario | Time Estimate | Cache State |
|----------|---------------|-------------|
| First run (no cache) | 2-3 minutes | Cold |
| Second run (file cache) | 20-40 seconds | Warm |
| Same session (memory cache) | 5-10 seconds | Hot |
| Partial cache (30% cached) | 1-2 minutes | Mixed |

**Performance Breakdown**:
- Fetch dependency files: 30-60 seconds (GitHub API, 50 repos)
- Parse dependencies: <5 seconds (local processing)
- Check versions (cold): 1.5-2 minutes (registry APIs, ~500 packages)
- Check versions (warm): <10 seconds (file cache)

**Optimization Tips**:
1. Use aggressive caching (7-day TTL)
2. Limit to top 5 ecosystems (95% coverage)
3. Parallel processing (5-10 threads)
4. Connection pooling (requests.Session)
5. Skip API calls for packages without version specifiers

---

## 9. Required Python Dependencies

### 9.1 Core Dependencies

```python
# requirements.txt additions

# Version parsing and comparison
packaging>=23.0           # Semantic version parsing (RECOMMENDED)

# HTTP requests with connection pooling
requests>=2.31.0          # Already present in project

# TOML parsing for pyproject.toml
tomli>=2.0.0; python_version < '3.11'  # Built into Python 3.11+

# Parallel processing (built-in)
# concurrent.futures (standard library)

# Caching (already implemented)
# Using existing APICache in src/spark/cache.py
```

### 9.2 Optional Dependencies

```python
# Optional: For advanced TOML parsing
toml>=0.10.2              # Alternative to tomli

# Optional: For XML parsing (Maven pom.xml)
lxml>=4.9.0               # Faster XML parsing than built-in xml.etree
```

### 9.3 Final Recommended requirements.txt

```plaintext
# GitHub Stats Spark - Production Dependencies
# Core dependencies
PyGithub>=2.1.1
PyYAML>=6.0.1
svgwrite>=1.4.3
requests>=2.31.0
python-dateutil>=2.8.2

# Dependency analysis (NEW)
packaging>=23.0
tomli>=2.0.0; python_version < '3.11'
```

---

## 10. Implementation Roadmap

### Phase 1: MVP (Week 1)
**Goal**: Support NPM + PyPI with basic caching

- Implement `DependencyParser` with NPM and PyPI support
- Implement `VersionChecker` with NPM/PyPI registry APIs
- Implement file-based caching using existing `APICache`
- Test with 10 sample repositories

**Deliverables**:
- `src/spark/dependencies/parser.py`
- `src/spark/dependencies/version_checker.py`
- Unit tests for parsing and version checking

### Phase 2: Multi-Ecosystem Support (Week 2)
**Goal**: Add Maven, RubyGems, Go support

- Extend `DependencyParser` for additional ecosystems
- Add registry APIs for Maven, RubyGems, Go
- Implement error handling for unsupported ecosystems
- Test with 30 repositories across multiple ecosystems

**Deliverables**:
- Enhanced parser with 5 ecosystems
- Comprehensive error handling
- Integration tests

### Phase 3: Performance Optimization (Week 3)
**Goal**: Achieve <3 minutes for 50 repositories

- Implement parallel processing
- Add in-memory caching layer
- Optimize API calls with connection pooling
- Benchmark and tune

**Deliverables**:
- `ParallelDependencyAnalyzer`
- Performance benchmarks
- Optimization documentation

### Phase 4: Integration & Polish (Week 4)
**Goal**: Integrate with main report generation

- Integrate dependency analysis into report generator
- Add markdown formatting for dependency info
- Implement GitHub Actions caching
- Final testing with 50+ repositories

**Deliverables**:
- Fully integrated feature
- Updated markdown report templates
- CI/CD pipeline updates

---

## 11. Testing Strategy

### 11.1 Unit Tests

```python
# tests/unit/test_dependency_parser.py

def test_parse_package_json():
    """Test package.json parsing."""
    parser = DependencyParser()

    content = '''
    {
      "dependencies": {
        "react": "^18.0.0",
        "express": "~4.18.0"
      },
      "devDependencies": {
        "jest": ">=29.0.0"
      }
    }
    '''

    deps = parser._parse_package_json(content)

    assert len(deps) == 3
    assert any(d['name'] == 'react' and d['version'] == '^18.0.0' for d in deps)
    assert any(d['name'] == 'express' for d in deps)
    assert any(d['name'] == 'jest' for d in deps)

def test_parse_requirements_txt():
    """Test requirements.txt parsing."""
    parser = DependencyParser()

    content = '''
    requests==2.31.0
    PyGithub>=2.1.0
    # This is a comment
    flask~=2.3.0
    -e git+https://github.com/user/repo.git#egg=package
    '''

    deps = parser._parse_requirements_txt(content)

    assert len(deps) == 3  # Should skip comment and editable
    assert any(d['name'] == 'requests' and d['version'] == '2.31.0' for d in deps)
    assert any(d['name'] == 'PyGithub' for d in deps)

def test_version_comparison():
    """Test semantic version comparison."""
    checker = VersionChecker()

    # Test major versions behind
    assert checker.calculate_versions_behind("1.0.0", "3.0.0") == 2
    assert checker.calculate_versions_behind("2.5.0", "2.8.0") == 0
    assert checker.calculate_versions_behind("^1.5.0", "3.2.1") == 2

    # Test status determination
    assert checker.determine_status(0) == 'current'
    assert checker.determine_status(1) == 'outdated'
    assert checker.determine_status(2) == 'outdated'
    assert checker.determine_status(3) == 'very outdated'
```

### 11.2 Integration Tests

```python
# tests/integration/test_dependency_analysis.py

def test_analyze_real_repository():
    """Test analyzing a real repository."""
    github = Github()  # No auth for testing
    analyzer = RepositoryDependencyAnalyzer(github)

    # Test with a known public repo
    result = analyzer.analyze_repository("facebook", "react")

    assert result['has_dependencies'] == True
    assert len(result['dependencies']) > 0
    assert 'summary' in result

def test_parallel_analysis():
    """Test parallel repository analysis."""
    github = Github()
    analyzer = ParallelDependencyAnalyzer(github, max_workers=3)

    repos = ["react", "vue", "angular"]
    results = analyzer.analyze_repositories("popular", repos)

    assert len(results) == 3
    for repo in repos:
        assert repo in results
```

### 11.3 Performance Tests

```python
# tests/performance/test_cache_performance.py

import time

def test_cache_speedup():
    """Test that caching provides significant speedup."""
    checker = VersionChecker()

    packages = [("npm", "react"), ("pypi", "requests"), ("npm", "express")]

    # First run (cold cache)
    start = time.time()
    for ecosystem, package in packages:
        checker.get_latest_version(ecosystem, package)
    cold_time = time.time() - start

    # Second run (warm cache)
    start = time.time()
    for ecosystem, package in packages:
        checker.get_latest_version(ecosystem, package)
    warm_time = time.time() - start

    # Cache should provide at least 10x speedup
    assert cold_time > warm_time * 10
```

---

## 12. Summary and Recommendations

### 12.1 Recommended Architecture

**Component Stack**:
1. **Dependency Parsing**: Custom parsers for package.json, requirements.txt, pyproject.toml, Gemfile, go.mod
2. **Version Checking**: Direct registry APIs (NPM, PyPI, RubyGems, Go Proxy)
3. **Version Comparison**: `packaging.version.Version` library
4. **Caching**: Hybrid file + in-memory cache (7-day TTL)
5. **Error Handling**: Graceful degradation with detailed error reporting
6. **Performance**: Parallel processing with 5 threads

**Supported Ecosystems** (Priority Order):
1. NPM (JavaScript/TypeScript) - PRIMARY
2. PyPI (Python) - PRIMARY
3. RubyGems (Ruby) - SECONDARY
4. Go Modules - SECONDARY
5. Maven (Java) - TERTIARY

**Expected Coverage**: 95% of repositories with dependency files

### 12.2 Performance Targets

| Metric | Target | Expected |
|--------|--------|----------|
| First run (50 repos) | <3 minutes | 2-3 minutes |
| Cached run (50 repos) | <1 minute | 20-40 seconds |
| API calls (first run) | <1000 | 500-800 |
| API calls (cached) | <100 | 0-50 |
| Accuracy (version detection) | >90% | 95% |

### 12.3 Required Dependencies

**Minimal Set**:
```plaintext
packaging>=23.0
tomli>=2.0.0; python_version < '3.11'
```

**Already Present**:
```plaintext
requests>=2.31.0
PyGithub>=2.1.1
```

### 12.4 Implementation Checklist

- [ ] Implement `DependencyParser` with NPM + PyPI support
- [ ] Implement `VersionChecker` with registry APIs
- [ ] Extend `APICache` for version data caching
- [ ] Add version comparison using `packaging.version`
- [ ] Implement `RepositoryDependencyAnalyzer`
- [ ] Add error handling for unsupported ecosystems
- [ ] Implement parallel processing
- [ ] Add unit tests for parsing and version checking
- [ ] Add integration tests with real repositories
- [ ] Benchmark performance and optimize
- [ ] Update requirements.txt
- [ ] Update GitHub Actions workflow for cache persistence
- [ ] Document API usage and caching strategy

### 12.5 Risk Mitigation

**Risk**: API rate limiting
**Mitigation**: Aggressive caching (7-day TTL), connection pooling, parallel processing limit

**Risk**: Unsupported package ecosystems
**Mitigation**: Graceful degradation, clear error messages, support top 5 ecosystems (95% coverage)

**Risk**: Parsing failures
**Mitigation**: Regex-based parsing with fallback, detailed error logging

**Risk**: Performance degradation
**Mitigation**: Parallel processing, hybrid caching, skip packages without versions

### 12.6 Success Criteria

1. Successfully analyzes 50 repositories in <3 minutes
2. Correctly identifies dependencies in 95% of standard dependency files
3. Accurately determines version currency within 2 major versions
4. Handles errors gracefully without crashing
5. Provides actionable summaries for each repository
6. Works in GitHub Actions without modifications

---

## Appendix A: API Reference Quick Guide

### NPM Registry
```bash
curl https://registry.npmjs.org/react
# Returns: { "dist-tags": { "latest": "18.2.0" }, ... }
```

### PyPI
```bash
curl https://pypi.org/pypi/requests/json
# Returns: { "info": { "version": "2.31.0" }, ... }
```

### RubyGems
```bash
curl https://rubygems.org/api/v1/gems/rails.json
# Returns: { "version": "7.1.2", ... }
```

### Go Proxy
```bash
curl https://proxy.golang.org/github.com/pkg/errors/@latest
# Returns: { "Version": "v0.9.1", "Time": "..." }
```

---

## Appendix B: Sample Output

### Repository Dependency Analysis Result

```json
{
  "repo": "awesome-project",
  "has_dependencies": true,
  "dependencies": [
    {
      "name": "react",
      "version": "^17.0.0",
      "ecosystem": "npm",
      "latest_version": "18.2.0",
      "status": "outdated",
      "versions_behind": 1
    },
    {
      "name": "express",
      "version": "~4.18.0",
      "ecosystem": "npm",
      "latest_version": "4.18.2",
      "status": "current",
      "versions_behind": 0
    },
    {
      "name": "requests",
      "version": "2.28.0",
      "ecosystem": "pypi",
      "latest_version": "2.31.0",
      "status": "current",
      "versions_behind": 0
    }
  ],
  "errors": [],
  "summary": "Mostly current (67% up-to-date, 3 dependencies)"
}
```

---

**Document Version**: 1.0
**Last Updated**: 2025-12-29
**Author**: GitHub Stats Spark Research
